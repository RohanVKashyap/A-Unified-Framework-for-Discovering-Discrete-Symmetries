{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 17:00:39.814408: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.constraints import maxnorm, nonneg, unit_norm\n",
    "import gc\n",
    "from argparse import ArgumentParser\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, Conv1D, Activation, Lambda, Permute, Conv2D, Flatten\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from itertools import permutations\n",
    "\n",
    "from math import comb\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import Ridge\n",
    "import time\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_Zk_Zn(x, indices):\n",
    "    def inv1(a, b):\n",
    "        return a * b ** 2\n",
    "\n",
    "    unstacked_variables  = tf.unstack(x, axis=1)\n",
    "    unstacked_variables = tf.gather(unstacked_variables, indices)\n",
    "\n",
    "    q1 = 0\n",
    "    for i in np.arange(len(indices)-1):\n",
    "        q1 += inv1(unstacked_variables[i], unstacked_variables[i+1])\n",
    "    q1 += inv1(unstacked_variables[len(indices)-1], unstacked_variables[0])        \n",
    "    return q1\n",
    "\n",
    "def apply_layers(x, layers):\n",
    "    for l in layers:\n",
    "        x = l(x)\n",
    "    return x\n",
    "\n",
    "def sigmaPi(fin, m, n, p):\n",
    "    fin = tf.transpose(fin, (0, 2, 1, 3))\n",
    "    fin = fin[:, :, tf.newaxis]\n",
    "    fin = tf.tile(fin, (1, 1, m, 1, 1))\n",
    "    y = fin @ p\n",
    "    y = tf.linalg.diag_part(y)\n",
    "    y = tf.reduce_prod(y, axis=3)\n",
    "    y = tf.reduce_sum(y, axis=2)\n",
    "    return y\n",
    "\n",
    "def prepare_permutation_matices(perm, n, m):\n",
    "    p1 = np.eye(n, dtype=np.float32)\n",
    "    p = np.tile(p1[np.newaxis], (m, 1, 1))\n",
    "    for i, x in enumerate(perm):\n",
    "        p[i, x, :] = p1[np.arange(n)]\n",
    "    return p \n",
    "            \n",
    "def get_matrix(d):\n",
    "    I = np.eye(d)\n",
    "    M1 = np.vstack([I]*((d-1)))\n",
    "    P = np.roll(I,1,axis=-1)\n",
    "    M2 = P@I\n",
    "    P_ = P\n",
    "    for i in range(1,d-1):\n",
    "        P1 = P_@P\n",
    "        M2 = np.vstack((M2,P1@I))\n",
    "        P_ = P1\n",
    "    return M1,M2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupInvariance(tf.keras.Model):\n",
    "    def __init__(self, perm, num_features):\n",
    "        super(GroupInvariance, self).__init__()\n",
    "        activation=tf.keras.activations.tanh\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.n = len(perm[0])\n",
    "        self.m = len(perm)\n",
    "        self.p = prepare_permutation_matices(perm, self.n, self.m)\n",
    "\n",
    "        self.features = [\n",
    "            tf.keras.layers.Dense(16, activation),\n",
    "            tf.keras.layers.Dense(64, activation),\n",
    "            tf.keras.layers.Dense(self.n * self.num_features, tf.keras.activations.sigmoid),\n",
    "            #tf.keras.layers.Dense(self.n * self.num_features, None),\n",
    "        ]\n",
    "\n",
    "        self.fc = [\n",
    "            #tf.keras.layers.Dense(32, tf.keras.activations.tanh),\n",
    "            tf.keras.layers.Dense(32, tf.keras.activations.relu, use_bias=False),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[:, :, tf.newaxis]\n",
    "        x = apply_layers(x, self.features)\n",
    "        x = tf.reshape(x, (-1, self.n, self.num_features, self.n))\n",
    "        x = sigmaPi(x, self.m, self.n, self.p)\n",
    "        x = apply_layers(x, self.fc)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(x, train_indices):\n",
    "    P = np.zeros((d, d))\n",
    "    p_indices = np.roll(train_indices, 1)\n",
    "    j = 0\n",
    "    for i in range(d):\n",
    "        if i in train_indices:\n",
    "            P[i][p_indices[j]] = 1\n",
    "            j += 1\n",
    "        else:\n",
    "            P[i][i] = 1\n",
    "\n",
    "    c_train_ds = []\n",
    "    P1 = P\n",
    "    for i in range(len(train_indices)-1):\n",
    "        c_train_ds.append(np.dot(P1, x))\n",
    "        P1 = P@P1\n",
    "\n",
    "    d_train_ds = []\n",
    "    P1 = np.eye(d)\n",
    "    id0 = train_indices[0]\n",
    "    id1 = train_indices[1]\n",
    "    P1[[id0,id1]] = P1[[id1,id0]]\n",
    "\n",
    "    for i in range(len(train_indices)-1):\n",
    "        d_train_ds.append(np.dot(P1, x))\n",
    "        P1 = P@P1        \n",
    "\n",
    "    p_indices = np.array(list(permutations(train_indices)))\n",
    "    random_indices = list(np.random.choice(len(p_indices), size=2*k, replace=False))\n",
    "    new_indices = p_indices[random_indices]\n",
    "\n",
    "    p_train_ds = []\n",
    "    for i in range(2*k):\n",
    "        y = x.copy()\n",
    "        y[train_indices] = y[new_indices[i]]\n",
    "        p_train_ds.append(y)\n",
    "\n",
    "    return np.array(c_train_ds), np.array(d_train_ds), np.array(p_train_ds)\n",
    "\n",
    "\n",
    "def get_data_v1(x, train_indices):\n",
    "    P = np.zeros((d, d))\n",
    "    p_indices = np.roll(train_indices, 1)\n",
    "    j = 0\n",
    "    for i in range(d):\n",
    "        if i in train_indices:\n",
    "            P[i][p_indices[j]] = 1\n",
    "            j += 1\n",
    "        else:\n",
    "            P[i][i] = 1\n",
    "\n",
    "    \n",
    "    P = P.T\n",
    "    c_train_ds = [np.dot(x, P)]\n",
    "    for i in range(len(train_indices)-2):\n",
    "        c_train_ds.append(np.dot(c_train_ds[-1], P))\n",
    "\n",
    "    \n",
    "    P1 = np.eye(d)\n",
    "    id0 = train_indices[0]\n",
    "    id1 = train_indices[1]\n",
    "    P1[[id0,id1]] = P1[[id1,id0]]\n",
    "    d_train_ds = [np.dot(x, P1)]\n",
    "    for i in range(len(train_indices)-1):\n",
    "        d_train_ds.append(np.dot(d_train_ds[-1], P))      \n",
    "\n",
    "    p_indices = np.array(list(permutations(train_indices)))\n",
    "    random_indices = list(np.random.choice(len(p_indices), size=2*k, replace=False))\n",
    "    new_indices = p_indices[random_indices]\n",
    "\n",
    "    p_train_ds = []\n",
    "    for i in range(2*k):\n",
    "        y = x.copy()\n",
    "        y[:,train_indices] = y[:,new_indices[i]]\n",
    "        p_train_ds.append(y)\n",
    "\n",
    "    return np.concatenate(c_train_ds, axis=0), np.concatenate(d_train_ds, axis=0), np.concatenate(p_train_ds, axis=0)\n",
    "\n",
    "\n",
    "def create_data(d, k, batch_size, true_indices, aux = True):\n",
    "    train_ds = np.random.rand(ts*batch_size, d)\n",
    "    val_ds = np.random.rand(vs*batch_size, d)\n",
    "    indices = np.array(true_indices).astype(np.int64)\n",
    "\n",
    "    # Additional data\n",
    "    if aux == True:\n",
    "        c_train_ds, d_train_ds, p_train_ds = get_data_v1(train_ds[0:5], true_indices)\n",
    "        train_ds = np.vstack((train_ds, c_train_ds, d_train_ds, p_train_ds))\n",
    "\n",
    "        c_val_ds, d_val_ds, p_val_ds = get_data_v1(val_ds[0:5], true_indices)\n",
    "        val_ds = np.vstack((val_ds, c_val_ds, d_val_ds, p_val_ds))\n",
    " \n",
    "    train_y = poly_Zk_Zn(train_ds, indices).numpy()\n",
    "    val_y = poly_Zk_Zn(val_ds, indices).numpy()\n",
    "    print(\"Shape info:\", [train_ds.shape, train_y.shape, val_ds.shape, val_y.shape])\n",
    "    return train_ds, train_y, val_ds, val_y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 17:01:04.157673: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-05-09 17:01:04.198753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:17:00.0 name: RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.54GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2023-05-09 17:01:04.198808: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-05-09 17:01:04.203116: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-05-09 17:01:04.203206: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-05-09 17:01:04.204565: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-05-09 17:01:04.204921: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-05-09 17:01:04.206194: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2023-05-09 17:01:04.207263: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-05-09 17:01:04.207456: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-05-09 17:01:04.211118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-05-09 17:01:04.211625: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 17:01:04.216131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:17:00.0 name: RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.54GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2023-05-09 17:01:04.219052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2023-05-09 17:01:04.219102: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-05-09 17:01:04.770135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-05-09 17:01:04.770164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2023-05-09 17:01:04.770171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2023-05-09 17:01:04.774492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 46721 MB memory) -> physical GPU (device: 0, name: RTX A6000, pci bus id: 0000:17:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape info: [(159, 10), (159,), (575, 10), (575,)]\n",
      "k: 5\n",
      "True Indices: [0, 2, 3, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1024)\n",
    "ts = 64\n",
    "vs = 480\n",
    "d = 10\n",
    "divisors = [1, 2, 5, 10]\n",
    "k = 5   #args.k\n",
    "\n",
    "true_indices = np.sort(np.random.choice(list(range(0, d)), k, replace=False)).tolist()\n",
    "train_ds, train_y, val_ds, val_y = create_data(d, k, 1, true_indices,True)\n",
    "print('k:',k)\n",
    "print('True Indices:',true_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_regularizer(x):\n",
    "    #x = tf.abs(x) + 1e-8\n",
    "    x = x/(tf.reduce_sum(x, axis=0))\n",
    "    entropy = tf.reduce_mean(tf.reduce_sum(-x*tf.math.log(x), axis=0))\n",
    "    return 1e-5 * entropy\n",
    "\n",
    "lambda_val = 1e-02\n",
    "l2_reg = tf.keras.regularizers.l2(1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicGroupInvarianceDiscover(tf.keras.Model):\n",
    "    def __init__(self, d):\n",
    "        super(CyclicGroupInvarianceDiscover, self).__init__()\n",
    "        activation = tf.keras.activations.tanh\n",
    "        activation2 = tf.keras.activations.tanh\n",
    "\n",
    "        self.d = d\n",
    "\n",
    "        self.linear1 = Dense(d,activation=None)\n",
    "        self.inputs1 = Dense(d * (d-1),use_bias=True,activation=None)\n",
    "        self.inputs2 = Dense(d * (d-1),use_bias=True,activation=None)\n",
    "        self.linear2 = Dense(d * (d-1),activation=None)\n",
    "\n",
    "        self.features = [\n",
    "            tf.keras.layers.Dense(16, activation, kernel_regularizer=l2_reg),\n",
    "            tf.keras.layers.Dense(32, activation, kernel_regularizer=l2_reg),\n",
    "            tf.keras.layers.Dense(32, activation, kernel_regularizer=l2_reg),\n",
    "        ]\n",
    "\n",
    "        self.Add = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])))\n",
    "        self.fc = [\n",
    "            tf.keras.layers.Dense(32, activation2,  kernel_regularizer=l2_reg),\n",
    "            tf.keras.layers.Dense(32, activation2,  kernel_regularizer=l2_reg),\n",
    "            #tf.keras.layers.Dense(32, activation2,  kernel_regularizer=l2_reg),\n",
    "            #tf.keras.layers.Dense(32, activation2,  kernel_regularizer=l2_reg),    \n",
    "            tf.keras.layers.Dense(1, kernel_regularizer=l2_reg),\n",
    "        ]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputs = self.linear1(inputs)                   # (B, d)\n",
    "        in1 = self.inputs1(inputs)                      # (B, d*(d-1))\n",
    "        in1 = self.linear2(in1)[:, :, tf.newaxis]       # (B, d*(d-1), 1)\n",
    "        in2 = self.inputs2(inputs)                      # (B, d*(d-1))\n",
    "        in2 = self.linear2(in2)[:, :, tf.newaxis]       # (B, d*(d-1), 1)\n",
    "\n",
    "        #print(\"in1\\n:\", in1)\n",
    "        #print(\"in2\\n:\", in2)\n",
    "        x = tf.concat((in1,in2), axis=-1)               # (B, d*(d-1), 2)             \n",
    "        x = apply_layers(x, self.features)              # (B, d*(d-1), 128) \n",
    "        x = self.Add(x)                                 # (B,  128) \n",
    "        x = apply_layers(x, self.fc)                    # (B,  1)   \n",
    "        return x               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicGroupInvarianceDiscoverPart(tf.keras.Model):\n",
    "    def __init__(self, d):\n",
    "        super(CyclicGroupInvarianceDiscoverPart, self).__init__()\n",
    "        activation = tf.keras.activations.tanh\n",
    "        self.d = d\n",
    "\n",
    "        self.linear1 = Dense(d,activation=None)\n",
    "        self.inputs1 = Dense(d * (d-1),use_bias=True,activation=None)\n",
    "        self.inputs2 = Dense(d * (d-1),use_bias=True,activation=None)\n",
    "        self.linear2 = Dense(d * (d-1),activation=None)\n",
    "\n",
    "        self.features = [\n",
    "            tf.keras.layers.Dense(16, activation, kernel_regularizer=l2_reg),\n",
    "            tf.keras.layers.Dense(64, activation, kernel_regularizer=l2_reg),\n",
    "            tf.keras.layers.Dense(128, activation, kernel_regularizer=l2_reg),\n",
    "        ]\n",
    "\n",
    "        self.Add = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])))\n",
    "        self.fc = [\n",
    "            tf.keras.layers.Dense(64, tf.keras.activations.tanh,  kernel_regularizer=l2_reg),\n",
    "            tf.keras.layers.Dense(64, tf.keras.activations.tanh,  kernel_regularizer=l2_reg),\n",
    "            tf.keras.layers.Dense(32, tf.keras.activations.tanh,  kernel_regularizer=l2_reg),\n",
    "            tf.keras.layers.Dense(32, tf.keras.activations.tanh,  kernel_regularizer=l2_reg),            \n",
    "            tf.keras.layers.Dense(1, kernel_regularizer=l2_reg),\n",
    "        ]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputs = self.linear1(inputs)                   # (B, d)\n",
    "        in1 = self.inputs1(inputs)                      # (B, d*(d-1))\n",
    "        in1 = self.linear2(in1)[:, :, tf.newaxis]       # (B, d*(d-1), 1)\n",
    "        in2 = self.inputs2(inputs)                      # (B, d*(d-1))\n",
    "        in2 = self.linear2(in2)[:, :, tf.newaxis]       # (B, d*(d-1), 1)\n",
    "\n",
    "        print(\"in1\\n:\", in1)\n",
    "        print(\"in2\\n:\", in2)\n",
    "        x = tf.concat((in1,in2), axis=-1)               # (B, d*(d-1), 2)             \n",
    "        x = apply_layers(x, self.features)              # (B, d*(d-1), 128) \n",
    "        x = self.Add(x)                                 # (B,  128) \n",
    "        x = apply_layers(x, self.fc)                    # (B,  1)  \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_matrix(k,d,indices,subgroup_indices):\n",
    "    if subgroup_indices == 0:\n",
    "        M3 = np.zeros((d,d))\n",
    "        for row,index in zip(np.arange(k),indices):\n",
    "            M3[row,index] = 1                                       #L1, Sk\n",
    "\n",
    "        M4 = np.eye(d*(d-1),d*(d-1))                                #L2, Sk     \n",
    "\n",
    "    elif subgroup_indices == 1 or 2:\n",
    "        l = d//k\n",
    "        I_k = np.zeros((k,d))\n",
    "        for row,index in zip(np.arange(k),indices):\n",
    "            I_k[row,index] = 1\n",
    "        M3 = np.vstack((I_k,)*l)                                    #L1, D2k or Zk\n",
    "\n",
    "        if subgroup_indices == 1:\n",
    "            I4 = np.eye(d)\n",
    "            M4 = np.vstack((I4,np.zeros(((d**2)-3*d,d)),I4))\n",
    "            M4 = np.hstack((M4,np.zeros(((d*(d-1),(d**2)-2*d)))))   #L2, D2k\n",
    "\n",
    "        elif subgroup_indices == 2:\n",
    "            I4 = np.eye(d)\n",
    "            M4 = np.vstack((I4,np.zeros(((d**2)-2*d,d))))\n",
    "            M4 = np.hstack((M4,np.zeros((d*(d-1),(d**2)-2*d))))     #L2, Zk  \n",
    "\n",
    "    return M3,M4\n",
    "\n",
    "def new_matrix_all(k,d,indices,subgroup_indices):\n",
    "    M3 = np.zeros((d,d))\n",
    "    for row,index in zip(np.arange(k),indices):\n",
    "        M3[row,index] = 1     \n",
    "\n",
    "    if(k>1):\n",
    "        if subgroup_indices == 0:                                \n",
    "            M4 = np.eye(d*(d-1),d*(d-1))                                #L2, Sk     \n",
    "\n",
    "        elif subgroup_indices == 1 or 2:    #D2k or Zk\n",
    "            M4 = np.zeros((d*(d-1),d*(d-1)))\n",
    "            M4[0:k-1,0:k-1] = np.eye(k-1)\n",
    "            key_loc_Zk = ((d-k)*d) + k - 1\n",
    "            M4[k-1, key_loc_Zk] = 1\n",
    "            if subgroup_indices == 1:\n",
    "                M4[k:(2*k)-1, -d+1:-d+k] = np.eye(k-1)\n",
    "                key_loc_D2k = ((k-2)*d)\n",
    "                M4[(2*k)-1, key_loc_D2k] = 1\n",
    "    else:\n",
    "        M4 =  np.zeros((d*(d-1),d*(d-1)))\n",
    "        M4[0,0] = 1               \n",
    "\n",
    "    return M3,M4\n",
    "\n",
    "\n",
    "def generate_matrix(k_,subgroup_indices):\n",
    "    #k_=int(np.random.choice(divisors,1,replace=False))\n",
    "    train_indices=np.sort(np.random.choice(list(range(0,d)),k_,replace=False))\n",
    "    #Generate matrix\n",
    "    L1, L2 = new_matrix(len(train_indices),d,train_indices,subgroup_indices)\n",
    "    #print('Matrix:',matrix)\n",
    "    return L1, L2, train_indices\n",
    "\n",
    "def generate_matrix_given_indices(train_indices,subgroup_indices):\n",
    "    #Generate matrix\n",
    "    L1, L2 = new_matrix(len(train_indices),d,train_indices,subgroup_indices)\n",
    "    #print('Matrix:',matrix)\n",
    "    return L1, L2   \n",
    "\n",
    "def generate_matrix_all(k_,subgroup_indices):\n",
    "    #k_=int(np.random.choice(divisors,1,replace=False))\n",
    "    train_indices=np.sort(np.random.choice(list(range(0,d)),k_,replace=False))\n",
    "    #Generate matrix\n",
    "    L1, L2 = new_matrix_all(len(train_indices),d,train_indices,subgroup_indices)\n",
    "    #print('Matrix:',matrix)\n",
    "    return L1, L2, train_indices\n",
    "\n",
    "def generate_matrix_given_indices_all(train_indices,subgroup_indices):\n",
    "    #Generate matrix\n",
    "    L1, L2 = new_matrix_all(len(train_indices),d,train_indices,subgroup_indices)\n",
    "    #print('Matrix:',matrix)\n",
    "    return L1, L2   \n",
    "\n",
    "def get_loss(M5,M6,epochs=100, batch_size=4):\n",
    "    Model_discover.set_weights(initial_weights)\n",
    "    #sample_output=Model_discover(val_ds)\n",
    "    bias0 = np.zeros(d)\n",
    "    bias3 = np.zeros(d*(d-1))\n",
    "\n",
    "    #Model_discover.layers[1].set_weights([M1_cc.T, bias_l1])\n",
    "    #Model_discover.layers[2].set_weights([M2_cc.T, bias_l2])\n",
    "    \n",
    "    Model_discover.layers[0].set_weights([M5.T, bias0])\n",
    "    Model_discover.layers[3].set_weights([M6.T, bias3])\n",
    "    \n",
    "    start=time.time()\n",
    "    train_history=Model_discover.fit(train_ds,train_y, \n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(val_ds,val_y),\n",
    "                        callbacks=[callback]) \n",
    "    end=time.time()\n",
    "    print('Time:',end-start)\n",
    "    \n",
    "    return train_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del Model_discover_part \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect() \n",
    "except:\n",
    "    print(\"model hasn't been yet defined\")\n",
    "\n",
    "Model_discover_part = CyclicGroupInvarianceDiscoverPart(d)\n",
    "M1, M2 = get_matrix(d)\n",
    "adam = Adam(learning_rate=1e-3)\n",
    "sample_output = Model_discover_part(val_ds)\n",
    "\n",
    "Model_discover_part.layers[0].trainable = False                    # Linear1\n",
    "Model_discover_part.layers[1].trainable = False\n",
    "Model_discover_part.layers[2].trainable = False\n",
    "Model_discover_part.layers[3].trainable = False                    # Linear 2\n",
    "\n",
    "Model_discover_part.compile(optimizer=adam, loss='mae')\n",
    "initial_weights=Model_discover_part.get_weights()\n",
    "Model_discover_part.set_weights(initial_weights)\n",
    "\n",
    "Model_discover_part.layers[1].set_weights([M1.T])\n",
    "Model_discover_part.layers[2].set_weights([M2.T])\n",
    "print(Model_discover_part.summary())\n",
    "\n",
    "\n",
    "#True Indices: [0, 2, 3, 6, 7]\n",
    "train_indices = np.array([0, 2, 3, 6, 7]).astype(np.int32)\n",
    "M5, M6  = generate_matrix_given_indices_all(train_indices,2) \n",
    "\n",
    "\n",
    "bias1 = np.zeros(d)\n",
    "bias2 = np.zeros(d*(d-1))\n",
    "Model_discover_part.layers[0].set_weights([M5.T, bias1])\n",
    "Model_discover_part.layers[3].set_weights([M6.T, bias2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'Pavan_May_7/saved_model.h5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True,)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model hasn't been yet defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 17:01:21.381445: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-05-09 17:01:22.084410: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-05-09 17:01:22.084468: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del Model_discover \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect() \n",
    "except:\n",
    "    print(\"model hasn't been yet defined\")\n",
    "\n",
    "Model_discover = CyclicGroupInvarianceDiscover(d)\n",
    "M1_cc, M2_cc = get_matrix(d)\n",
    "adam = Adam(learning_rate=1e-3)\n",
    "sample_output = Model_discover(val_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cyclic_group_invariance_discover\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  110       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  990       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  990       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  8190      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  48        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  33        \n",
      "=================================================================\n",
      "Total params: 14,073\n",
      "Trainable params: 3,793\n",
      "Non-trainable params: 10,280\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "Model_discover.layers[0].trainable = False                    # Linear1\n",
    "Model_discover.layers[1].trainable = False\n",
    "Model_discover.layers[2].trainable = False\n",
    "Model_discover.layers[3].trainable = False                    # Linear 2\n",
    "\n",
    "Model_discover.compile(optimizer=adam, loss='mae')\n",
    "\n",
    "\n",
    "bias_l1 = np.zeros(d*(d-1))\n",
    "bias_l2 = np.zeros(d*(d-1))\n",
    "\n",
    "Model_discover.layers[1].set_weights([M1_cc.T, bias_l1])\n",
    "Model_discover.layers[2].set_weights([M2_cc.T, bias_l2])\n",
    "\n",
    "initial_weights=Model_discover.get_weights()\n",
    "Model_discover.set_weights(initial_weights)\n",
    "\n",
    "print(Model_discover.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts for bandit arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [list(combinations(range(d), i)) for i in range(1, d+1)]\n",
    "C = [list(item) for sublist in C for item in sublist]\n",
    "\n",
    "At = np.zeros((len(C), d))\n",
    "for i, idx in zip(range(len(C)), C):\n",
    "    At[i][idx] = 1\n",
    "\n",
    "# Normalization\n",
    "#l2_norms_rows = LA.norm(At, 2, axis=1)[:, np.newaxis]\n",
    "#At = At/l2_norms_rows\n",
    "At = np.hstack((At, np.ones((At.shape[0], 1))))\n",
    "\n",
    "C_d = [list(combinations(range(d), i)) for i in divisors]\n",
    "C_d = [list(item) for sublist in C_d for item in sublist]\n",
    "\n",
    "d_At = np.zeros((len(C_d), d))\n",
    "for i, idx in zip(range(len(C_d)), C_d):\n",
    "    d_At[i][idx] = 1\n",
    "\n",
    "# Normalization\n",
    "#l2_norms_rows = LA.norm(d_At, 2, axis=1)[:, np.newaxis]\n",
    "#d_At = d_At/l2_norms_rows\n",
    "d_At = np.hstack((d_At, np.ones((d_At.shape[0], 1))))\n",
    "\n",
    "s_arms = np.hstack((At, np.array([[1, 0, 0]] * At.shape[0])))\n",
    "d_arms = np.hstack((d_At, np.array([[0, 1, 0]] * d_At.shape[0])))\n",
    "z_arms = np.hstack((d_At, np.array([[0, 0, 1]] * d_At.shape[0])))\n",
    "\n",
    "At = np.vstack((s_arms, d_arms, z_arms))\n",
    "print('At shape:',At.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R = 0.01\n",
    "R = 0.1\n",
    "epsilon = 0.5\n",
    "delta = 0.5\n",
    "n_features = d+4\n",
    "subgroup = {0:'Sk', 1:'D2k', 2:'Zk'}\n",
    "B = np.eye(n_features)\n",
    "B_inv = np.eye(n_features)\n",
    "f = np.zeros((n_features, 1))\n",
    "mu_hat = np.zeros((n_features, 1))\n",
    "arm_iterations = 450\n",
    "v = R * np.sqrt(24 / epsilon * n_features * np.log(1 / delta))\n",
    "contexts = At\n",
    "As = []\n",
    "b = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(arm_iterations):\n",
    "    print(\"Iteration:\",i)\n",
    "    k_=int(np.random.choice(divisors,1,replace=False))\n",
    "    subgroup_indices = 2 \n",
    "\n",
    "    L1, L2, train_indices = generate_matrix(k_,subgroup_indices) \n",
    "    contexts_t = np.zeros(10)\n",
    "    contexts_t[train_indices] = 1\n",
    "    train_history = get_loss(L1,L2)\n",
    "    min_loss = np.min(train_history.history['val_loss'])\n",
    "    reward = -min_loss\n",
    "\n",
    "    As.append(contexts_t)\n",
    "    b.append(min_loss)\n",
    "    print([min_loss,train_indices])\n",
    "\n",
    "    np.save('c_r_Zk_As.npy',np.array(As))\n",
    "    np.save('c_r_Zk_b.npy',np.array(b))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Model_discover \n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
