{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 11:38:08.673498: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "#import matplotlib\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, GRU, Conv1D, Activation, Lambda, Permute, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from keras.constraints import maxnorm, nonneg\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, CSVLogger, EarlyStopping, TensorBoard\n",
    "#from tensorflow.keras.utils.vis_utils import model_to_dot\n",
    "from tqdm import tqdm,trange\n",
    "import gc\n",
    "from argparse import ArgumentParser\n",
    "from itertools import product,combinations \n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from keras.constraints import maxnorm, nonneg\n",
    "import time\n",
    "\n",
    "from math import comb\n",
    "from sklearn.linear_model import Ridge\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape info: [(200000, 10, 784), dtype('float16'), (200000, 10)]\n"
     ]
    }
   ],
   "source": [
    "def save_pd(history, fileName):\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "    # save to json:  \n",
    "    hist_json_file = fileName + '_history.json' \n",
    "    with open(hist_json_file, mode='w') as f:\n",
    "        hist_df.to_json(f)\n",
    "\n",
    "    # save to csv: \n",
    "    hist_csv_file = fileName + '_history.csv'\n",
    "    with open(hist_csv_file, mode='w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "\n",
    "features = np.load(\"./feature.npy\")\n",
    "all_labels = np.load(\"./all_labels.npy\")\n",
    "print(\"shape info:\",[features.shape, features.dtype, all_labels.shape])\n",
    "\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "\n",
    "patience = 10\n",
    "\n",
    "def get_matrix(n):\n",
    "    I = np.eye(d)\n",
    "    M1 = np.vstack([I]*((d-1)))\n",
    "    P = np.roll(I,1,axis=-1)\n",
    "    M2 = P@I\n",
    "    P_ = P\n",
    "    for i in range(1,d-1):\n",
    "        P1 = P_@P\n",
    "        M2 = np.vstack((M2,P1@I))\n",
    "        P_ = P1\n",
    "    return M1,M2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic Model for image sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_model(d,l1_lambda):\n",
    "    l1_reg = tf.keras.regularizers.l1(l1_lambda) \n",
    "\n",
    "    input_img = Input(shape=(d, 28*28)) #Layer 0\n",
    "    #print('input_img:',input_img.shape)\n",
    "\n",
    "    input_img_p = Permute((2,1))(input_img) #Layer 1\n",
    "\n",
    "    linear1 = Dense(d,activation=None)(input_img_p) #Layer 2\n",
    "    inputs1 = Dense(d*(d-1),use_bias=False,activation=None)(linear1) #Layer 3\n",
    "    #print('inputs1:',inputs1.shape)\n",
    "    inputs2 = Dense(d*(d-1),use_bias=False,activation=None)(linear1) #Layer 4\n",
    "    #print('inputs2:',inputs2.shape)\n",
    "\n",
    "    x = tf.concat((inputs1,inputs2), axis=1)     #Layer 5 # (B, 2*H*W, d*(d-1))\n",
    "    linear2 = Dense(d*(d-1),activation=None)(x) #Layer 6\n",
    "    #new_dense2 = Dense(d*(d-1),use_bias=False,activation=None)(x)\n",
    "    x = Permute((2,1))(linear2) #layer 7 # (B, d*(d-1), 2*H*W)\n",
    "    #print('x shape:',x.shape)\n",
    "    \n",
    "    x = Dense(300, activation='tanh')(x)   \n",
    "    x = Dense(100, activation='tanh')(x)\n",
    "    x = Dense(30, activation='tanh')(x)\n",
    "    #print('x shape:',x.shape)\n",
    "    \n",
    "    Adder = Lambda(lambda x: K.sum(x, axis=1), output_shape=(lambda shape: (shape[0], shape[2])))\n",
    "    \n",
    "    x = Adder(x)\n",
    "    print('x shape:',x.shape)\n",
    "    \n",
    "    encoded = Dense(1)(x)\n",
    "    \n",
    "    summer = Model(input_img, encoded)\n",
    "    #adam = Adam(learning_rate=1e-3, epsilon=1e-3)\n",
    "    #summer.compile(optimizer=adam, loss='mae')\n",
    "    #summer.get_layer(index=1).set_weights([images])\n",
    "    return summer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******True Indices******: [1, 3, 4, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "num_train_examples = 140000\n",
    "num_val_examples = 25000\n",
    "num_test_examples = 30000\n",
    "max_train_length = 10\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "total_examples = num_train_examples + num_val_examples + num_test_examples\n",
    "d = 10\n",
    "k = 5\n",
    "divisors = [1, 2, 5, 10]\n",
    "#all_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "#all_indices = [0, 1, 2, 3]\n",
    "all_indices = np.sort(np.random.choice(list(range(0, d)), k, replace=False)).tolist()\n",
    "print('******True Indices******:',all_indices)\n",
    "basepath = \"./cyclic_discover_v1_Sk10_trails_1\"\n",
    "\n",
    "indices = all_indices\n",
    "labels = all_labels[:,np.array(indices)]\n",
    "labels = np.sum(labels,1)            \n",
    "\n",
    "train_data = (features[0:num_train_examples], labels[0:num_train_examples])\n",
    "\n",
    "val_data = (features[num_train_examples:num_train_examples+num_val_examples],\n",
    "            labels[num_train_examples:num_train_examples+num_val_examples])\n",
    "\n",
    "test_data = (features[num_train_examples+num_val_examples:],\n",
    "            labels[num_train_examples+num_val_examples:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_matrix_all(k,d,indices,subgroup_indices):\n",
    "    M3 = np.zeros((d,d))\n",
    "    for row,index in zip(np.arange(k),indices):\n",
    "        M3[row,index] = 1     \n",
    "\n",
    "    if(k>1):\n",
    "        if subgroup_indices == 0:                                \n",
    "            M4 = np.eye(d*(d-1),d*(d-1))                                #L2, Sk     \n",
    "\n",
    "        elif subgroup_indices == 1 or 2:    #D2k or Zk\n",
    "            M4 = np.zeros((d*(d-1),d*(d-1)))\n",
    "            M4[0:k-1,0:k-1] = np.eye(k-1)\n",
    "            key_loc_Zk = ((d-k)*d) + k - 1\n",
    "            M4[k-1, key_loc_Zk] = 1\n",
    "            if subgroup_indices == 1: #D2k\n",
    "                M4[k:(2*k)-1, -d+1:-d+k] = np.eye(k-1)\n",
    "                key_loc_D2k = ((k-2)*d)\n",
    "                M4[(2*k)-1, key_loc_D2k] = 1\n",
    "    else:\n",
    "        M4 =  np.zeros((d*(d-1),d*(d-1)))\n",
    "        M4[0,0] = 1               \n",
    "\n",
    "    return M3,M4\n",
    "\n",
    "def generate_matrix_all(k_,subgroup_indices):\n",
    "    #k_=int(np.random.choice(divisors,1,replace=False))\n",
    "    train_indices=np.sort(np.random.choice(list(range(0,d)),k_,replace=False))\n",
    "    #Generate matrix\n",
    "    L1, L2 = new_matrix_all(len(train_indices),d,train_indices,subgroup_indices)\n",
    "    #print('Matrix:',matrix)\n",
    "    return L1, L2, train_indices\n",
    "\n",
    "def generate_matrix_given_indices_all(train_indices,subgroup_indices):\n",
    "    #Generate matrix\n",
    "    L1, L2 = new_matrix_all(len(train_indices),d,train_indices,subgroup_indices)\n",
    "    #print('Matrix:',matrix)\n",
    "    return L1, L2   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'saved_model.h5'\n",
    "callback = tf.keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True,)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, verbose=1, patience=patience, min_lr=0.000001)\n",
    "\n",
    "def get_loss(M5,M6,epochs=5):\n",
    "    model_discover.set_weights(initial_weights)\n",
    "    #sample_output=Model_discover(val_ds)\n",
    "    bias1 = np.zeros(d)\n",
    "    bias2 = np.zeros(d*(d-1))\n",
    "    model_discover.layers[2].set_weights([M5.T,bias1])\n",
    "    model_discover.layers[6].set_weights([M6.T,bias2])\n",
    "\n",
    "    start=time.time()\n",
    "    train_history = model_discover.fit(train_data[0], \n",
    "                                        train_data[1], \n",
    "                                        epochs=epochs, \n",
    "                                        batch_size=128,\n",
    "                                        shuffle=True,\n",
    "                                        validation_data=val_data,\n",
    "                                        callbacks=[callback,reduce_lr]) \n",
    "    \n",
    "    end=time.time()\n",
    "    print('Time:',end-start)\n",
    "    \n",
    "    return train_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (None, 30)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model_discover \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect() \n",
    "except:\n",
    "    print(\"model hasn't been yet defined\")\n",
    "\n",
    "model_discover = cyclic_model(d,0.01)\n",
    "M1, M2 = get_matrix(d)\n",
    "\n",
    "sample_y = np.random.randn(1, 10, 784)\n",
    "sample_output = model_discover(sample_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 10, 784)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 784, 10)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 784, 10)      110         permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 784, 90)      900         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 784, 90)      900         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (None, 1568, 90)     0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1568, 90)     8190        tf.concat[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 90, 1568)     0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 90, 300)      470700      permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 90, 100)      30100       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 90, 30)       3030        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 30)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            31          lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 513,961\n",
      "Trainable params: 503,861\n",
      "Non-trainable params: 10,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_discover.layers[2].trainable = False\n",
    "model_discover.layers[3].trainable = False\n",
    "model_discover.layers[4].trainable = False\n",
    "model_discover.layers[6].trainable = False\n",
    "\n",
    "adam = Adam(learning_rate=1e-3, epsilon=1e-3)\n",
    "model_discover.compile(optimizer=adam, loss='mae')\n",
    "\n",
    "model_discover.layers[3].set_weights([M1.T])\n",
    "model_discover.layers[4].set_weights([M2.T])\n",
    "\n",
    "initial_weights = model_discover.get_weights()\n",
    "model_discover.set_weights(initial_weights)\n",
    "\n",
    "print(model_discover.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexts for bandit arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [list(combinations(range(d), i)) for i in range(1, d+1)]\n",
    "C = [list(item) for sublist in C for item in sublist]\n",
    "\n",
    "At = np.zeros((len(C), d))\n",
    "for i, idx in zip(range(len(C)), C):\n",
    "    At[i][idx] = 1\n",
    "\n",
    "At = np.hstack((At, np.ones((At.shape[0], 1))))\n",
    "\n",
    "C_d = [list(combinations(range(d), i)) for i in divisors]\n",
    "C_d = [list(item) for sublist in C_d for item in sublist]\n",
    "\n",
    "d_At = np.zeros((len(C_d), d))\n",
    "for i, idx in zip(range(len(C_d)), C_d):\n",
    "    d_At[i][idx] = 1\n",
    "\n",
    "d_At = np.hstack((d_At, np.ones((d_At.shape[0], 1))))\n",
    "\n",
    "s_arms = np.hstack((At, np.array([[1, 0, 0]] * At.shape[0])))\n",
    "d_arms = np.hstack((d_At, np.array([[0, 1, 0]] * d_At.shape[0])))\n",
    "z_arms = np.hstack((d_At, np.array([[0, 0, 1]] * d_At.shape[0])))\n",
    "\n",
    "At = np.vstack((s_arms, d_arms, z_arms))\n",
    "print('At shape:',At.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TS params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R = 0.01\n",
    "R = 0.1\n",
    "epsilon = 0.5\n",
    "delta = 0.5\n",
    "n_features = d+4\n",
    "subgroup = {0:'Sk', 1:'D2k', 2:'Zk'}\n",
    "B = np.eye(n_features)\n",
    "B_inv = np.eye(n_features)\n",
    "f = np.zeros((n_features, 1))\n",
    "mu_hat = np.zeros((n_features, 1))\n",
    "arm_iterations = 450\n",
    "v = R * np.sqrt(24 / epsilon * n_features * np.log(1 / delta))\n",
    "contexts = At\n",
    "As = []\n",
    "b = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(arm_iterations):\n",
    "    print(\"Iteration:\",i)\n",
    "    k_=int(np.random.choice(divisors,1,replace=False))\n",
    "    subgroup_indices = 2 \n",
    "\n",
    "    L1, L2, train_indices = generate_matrix(k_,subgroup_indices) \n",
    "    contexts_t = np.zeros(10)\n",
    "    contexts_t[train_indices] = 1\n",
    "    train_history = get_loss(L1,L2)\n",
    "    min_loss = np.min(train_history.history['val_loss'])\n",
    "    reward = -min_loss\n",
    "\n",
    "    As.append(contexts_t)\n",
    "    b.append(min_loss)\n",
    "    print([min_loss,train_indices])\n",
    "\n",
    "    np.save('c_r_Zk_As.npy',np.array(As))\n",
    "    np.save('c_r_Zk_b.npy',np.array(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_discover \n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
